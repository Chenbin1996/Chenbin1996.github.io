---
layout:     post
title:      Java基于webmagic框架进行爬取页面数据
subtitle:   webmagic的一个简单使用教程
date:       2018-04-08
author:     如漩涡
header-img: img/post-bg-android.jpg
catalog: true
tags:
    - Java
---

> 更多文章查看本人CSDN博客 [《如漩涡的博客》](https://blog.csdn.net/m0_37701381)

爬虫框架网络上一搜一大把，看个人觉得哪款爬虫框架比较适用自己，刚开始我接触使用的是xxl-crawler，经过几次测试实现，虽然这个框架不错，功能丰富，但跟我当时的需求来说并不满足，或许是我还没研究深，不太会用，时间有限，我选择了去找其他框架，后来接触了webmagic，操作简单方便，可以多次深入爬取，以及css，$，xpath都支持，还有获取单条数据或者多条数据的选择，正好符合我想要的，就着手研究了起来，可以自行去官网查看文档说明，中文官网地址：http://webmagic.io/docs/zh/

## 简单的数据爬取教程

### maven依赖

如果是maven项目或者Spring Boot项目话，在pom文件中加入以下依赖：

```xml
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-core</artifactId>
    <version>0.7.3</version>
</dependency>
<dependency>
    <groupId>us.codecraft</groupId>
    <artifactId>webmagic-extension</artifactId>
    <version>0.7.3</version>
</dependency>
```

### 代码操作
等待架包下载完成后，可以进一步开始爬取数据操作了，在这里我就简单爬取一下自己的博客地址作为例子：http://blog.csdn.net/m0_37701381

新建一个带有main方法的类：WebmagicTest 这个类要实现 PageProcessor这个接口，这个是webmagic提供的接口，须实现

实现后会有两个方法：
```java
@Override
public void process(Page page) {
    
}

@Override
public Site getSite() {
    return null;
}
```

process方法里是你抓取数据的返回集，存在了这个page里，page这个类里封装了很多方法，我就不一一介绍了，官网比我说的更详细，最后查看我全部的代码，我会带上注释，一一解释都有什么作用，就不在文章中多做墨水：
```java
import us.codecraft.webmagic.Page;
import us.codecraft.webmagic.Site;
import us.codecraft.webmagic.Spider;
import us.codecraft.webmagic.processor.PageProcessor;

import java.util.List;

public class WebmagicTest implements PageProcessor {
    //抓取网站的相关配置，包括编码、抓取间隔、重试次数等
    private Site site = Site.me().setRetryTimes(3).setSleepTime(1000);

    /**
     * 在这里编写抽取数据逻辑代码
     * @param page
     */
    @Override
    public void process(Page page) {

        //用以下这句话，可以抓取到前端页面class = "link_title"下的所有href链接
        //.all()是获取所有 .get()是获取单条
        List<String> href = page.getHtml().$(".link_title").links().all();

        //做个循环打印一下数据看看对不对
        //for (String s : href) {
        //System.out.println(s);
        //}
        //将抓取的href链接进行第二次爬取，是单条的话就用page.addTargetRequest()
        page.addTargetRequests(href);

        //做个判断，判断这次进来process里面的内容是哪个链接的
        //如果是第二次进来的链接，做第二次进来的事，不然做以上的事情
        //第二次进来后等同于我们手动点击了某一个文章链接
        //matches是String类里的一个正则表达式判断，第二次进来的页面前面部分和后面部分满足条件返回true
        //.*是表示前面或者的所有部分不包括特殊符号
        if (page.getUrl().toString().matches(".*/article.*")){
            //要是还有其他链接进入在这里执行page.addTargetRequests()
            //这里是第二次进入process方法，在这里抓取文章内的内容
            //分别抓取标题、作者、内容，首先要知道这个页面的属性、推荐chrome浏览器，按F12查看每个页面元素是否相同
            //这里演示用xpath获取，xpath可以在chrome中选中元素右键Copy-Copy Xpath，在后面加上/tidyText()去空格去HTML标签等
            String title = page.getHtml().xpath("//*[@id=\"article_details\"]/div[1]/h1/span/a/text()").get();
            String user = page.getHtml().xpath("//*[@id=\"blog_userface\"]/span/a/text()").get();
            String content = page.getHtml().xpath("//*[@id=\"article_content\"]/text()").get();

            System.out.println("打印标题：" + title);
            System.out.println("打印作者：" + user);
            System.out.println("打印文章内容：" + content);
        }

    }

    @Override
    public Site getSite() {
        /*这里的site是属性定义的*/
        return site;
    }

    public static void main(String[] args) {
        //Spider.create执行的时候默认是new了一个Spider对象
        //在create参数里面，把当前类对象给它,addUrl添加要抓取的网页，开始线程池，start是异步方法,run是同步
        //我这里选择异步方法(start)，好了，启动main方法的时候就已经去抓取了，接着就在process里面操作抽取
        Spider.create(new WebmagicTest()).addUrl("http://blog.csdn.net/m0_37701381").thread(5).start();
    }
}
```
另外介绍一下，webmagic的xpath有三个方法“allText()/text()/tidyText()”，个人总结：tidyText()比较适用于.all()；text()/allText()适用于单条数据.get()；使用就是在复制过来的xpath后面加上即可，代码中有写

这样就算是完成了，看需求去爬取，想相应的逻辑思维，抓取到的数据可以用上一篇文章介绍的文件输出流保存，也可以存到数据库里，看个人操作，这里就不多做介绍了，webmagic框架操作就这么几步非常方便